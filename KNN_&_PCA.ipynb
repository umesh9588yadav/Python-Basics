{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assginment 4 : KNN & PCA**"
      ],
      "metadata": {
        "id": "4URS-VudJeKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "\n",
        "#  ** K-Nearest Neighbors (KNN)**\n",
        "\n",
        "KNN is a **supervised machine-learning algorithm** used for **classification** and **regression**. It predicts outputs based on the idea that **similar data points are close to each other** in feature space.\n",
        "\n",
        "It is a **lazy learner**‚Äîmeaning it does not build a model during training. Instead, it stores the training data and makes predictions only when needed.\n",
        "\n",
        "\n",
        "#  **How KNN Works**\n",
        "\n",
        "For a new input point:\n",
        "\n",
        "1. Compute its distance to all points in the training dataset.\n",
        "2. Select the **K closest points** (neighbors).\n",
        "3. Make a prediction using those neighbors.\n",
        "\n",
        "Common distance metrics:\n",
        "\n",
        "* Euclidean (most common)\n",
        "* Manhattan\n",
        "* Minkowski\n",
        "* Cosine similarity\n",
        "\n",
        "\n",
        "#  **KNN for Classification**\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Find the **K nearest neighbors** of the new point.\n",
        "2. Each neighbor ‚Äúvotes‚Äù for its class label.\n",
        "3. The class with the **majority vote** becomes the prediction.\n",
        "\n",
        "Example:\n",
        "If K = 5 and among the neighbors 3 are Class A and 2 are Class B ‚Üí predicted class = **A**.\n",
        "\n",
        "\n",
        "#  **KNN for Regression**\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Find the **K nearest neighbors**.\n",
        "2. Instead of voting, take the **average value** (or weighted average) of their numerical labels.\n",
        "\n",
        "Example:\n",
        "Neighbor values = 10, 12, 14 ‚Üí prediction = (10 + 12 + 14) / 3 = **12**.\n",
        "\n",
        "**Weighted regression:** closer points have more influence by using weights like 1/distance.\n",
        "\n",
        "\n",
        "#  **Choosing the Value of K**\n",
        "\n",
        "* **Small K** ‚Üí more complex, risk of overfitting.\n",
        "* **Large K** ‚Üí smoother, risk of underfitting.\n",
        "* Typically selected using cross-validation.\n",
        "\n",
        "\n",
        "# **Strengths of KNN**\n",
        "\n",
        "* Very simple, easy to understand.\n",
        "* Works well for small/medium datasets.\n",
        "* Handles non-linear decision boundaries.\n",
        "* No training time needed.\n",
        "\n",
        "\n",
        "\n",
        "#  **Weaknesses of KNN**\n",
        "\n",
        "* Slow prediction on large datasets (computes many distances).\n",
        "* Sensitive to irrelevant features and differences in scale ‚Üí requires normalization.\n",
        "* Performs poorly in very high-dimensional spaces (curse of dimensionality).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "szRs4ho7JkqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "PIgzM9x2Nlws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "\n",
        "#  **Curse of Dimensionality?**\n",
        "\n",
        "The **Curse of Dimensionality** refers to a set of problems that arise when data has **too many features (dimensions)**. As the number of dimensions increases:\n",
        "\n",
        "* Data becomes **sparse** (spread out).\n",
        "* Distances between points become **less meaningful**.\n",
        "* Models that rely on distance or neighborhood relationships become less effective.\n",
        "\n",
        "In simple terms:\n",
        "**High-dimensional space makes it harder to find ‚Äúnearby‚Äù points.**\n",
        "\n",
        "\n",
        "#  **Why Does This Happen?**\n",
        "\n",
        "When you add more dimensions:\n",
        "\n",
        "1. **Space grows exponentially**, but the amount of data does not.\n",
        "\n",
        "   * You would need exponentially more data to cover the space effectively.\n",
        "\n",
        "2. **All points become far apart.**\n",
        "\n",
        "   * The difference between the nearest and farthest neighbors shrinks.\n",
        "\n",
        "3. **Distance metrics stop working well.**\n",
        "\n",
        "   * Euclidean distance becomes less discriminative.\n",
        "\n",
        "\n",
        "#  **How the Curse of Dimensionality Affects KNN**\n",
        "\n",
        "KNN heavily depends on **distance** to find the closest neighbors. In high dimensions, this becomes problematic.\n",
        "\n",
        "### **1. Distances lose meaning**\n",
        "\n",
        "When dimensionality is high:\n",
        "\n",
        "* Nearest neighbors are **not much closer** than farthest neighbors.\n",
        "* KNN struggles to identify truly similar points.\n",
        "\n",
        "This leads to **poor classification and regression accuracy**.\n",
        "\n",
        "\n",
        "### **2. Increased risk of overfitting**\n",
        "\n",
        "With many irrelevant features:\n",
        "\n",
        "* Noise dominates genuine patterns.\n",
        "* KNN may pick misleading neighbors because distance is distorted.\n",
        "\n",
        "\n",
        "### **3. Computational cost increases**\n",
        "\n",
        "More dimensions mean:\n",
        "\n",
        "* More distance calculations\n",
        "* Each distance computation becomes more expensive\n",
        "* Prediction time becomes slow\n",
        "\n",
        "Since KNN is a lazy learner, this slows down the algorithm significantly.\n",
        "\n",
        "\n",
        "### **4. Need for feature scaling and dimensionality reduction**\n",
        "\n",
        "To combat high-dimensional issues, techniques like:\n",
        "\n",
        "* **PCA (Principal Component Analysis)**\n",
        "* **Feature selection (e.g., removing irrelevant features)**\n",
        "* **Normalization/standardization**\n",
        "\n",
        "are often applied before using KNN.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5ANFj-7kJwui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "61stQhbnNsxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "\n",
        "#  **What Is Principal Component Analysis (PCA)?**\n",
        "\n",
        "**PCA** is an **unsupervised dimensionality reduction technique** that transforms high-dimensional data into a smaller set of **new variables** called **principal components**.\n",
        "\n",
        "These components:\n",
        "\n",
        "* Are **linear combinations** of the original features\n",
        "* Capture the **maximum variance** in the data\n",
        "* Are **uncorrelated** with each other\n",
        "* Are ordered:\n",
        "\n",
        "  * 1st component ‚Üí most variance\n",
        "  * 2nd component ‚Üí second-most variance\n",
        "  * ...and so on\n",
        "\n",
        "PCA is mainly used to:\n",
        "\n",
        "* Reduce dimensionality (to combat the curse of dimensionality)\n",
        "* Remove noise\n",
        "* Visualize high-dimensional data\n",
        "* Speed up algorithms that struggle with many features\n",
        "\n",
        "\n",
        "#  **How PCA Works (Conceptually)**\n",
        "\n",
        "1. Standardize the data.\n",
        "2. Compute the covariance matrix.\n",
        "3. Find its eigenvalues and eigenvectors.\n",
        "4. Sort components by variance explained.\n",
        "5. Select the top *k* components.\n",
        "6. Transform data into the new component space.\n",
        "\n",
        "The transformed features are **not original features**, but new axes that best describe the data's structure.\n",
        "\n",
        "\n",
        "#  **How PCA Differs from Feature Selection**\n",
        "\n",
        "PCA is a **feature extraction** method, not a selection method.\n",
        "\n",
        "| Aspect               | PCA (Feature Extraction)                                                   | Feature Selection                             |\n",
        "| -------------------- | -------------------------------------------------------------------------- | --------------------------------------------- |\n",
        "| **What it does**     | Creates new features (principal components) by combining original features | Chooses a subset of the existing features     |\n",
        "| **Output features**  | New, transformed, uncorrelated components                                  | Original features only                        |\n",
        "| **Interpretability** | Low ‚Äî components are combinations of variables                             | High ‚Äî uses actual features                   |\n",
        "| **Type**             | Unsupervised                                                               | Usually supervised (based on target variable) |\n",
        "| **Goal**             | Reduce dimensionality while preserving variance                            | Remove irrelevant or redundant features       |\n",
        "| **Changes data?**    | Yes ‚Äî transforms it                                                        | No ‚Äî keeps original feature meanings          |\n",
        "\n",
        "\n",
        "#  Example\n",
        "\n",
        "Suppose you have 10 correlated features.\n",
        "\n",
        "* **PCA** may create 3 new components that capture 95% of the variance.\n",
        "* **Feature selection** may choose only 3 of the original 10 features.\n",
        "\n",
        "Both reduce dimensionality, but in very different ways.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5z1CzLhRJ0MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "sZ66DdNON9fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "\n",
        "# **What Are Eigenvalues and Eigenvectors in PCA?**\n",
        "\n",
        "In PCA, we compute the **covariance matrix** of the data and then find its **eigenvalues** and **eigenvectors**.\n",
        "\n",
        "### **Eigenvectors**\n",
        "\n",
        "* Define the **directions** of the new feature axes (principal components).\n",
        "* Each eigenvector represents a direction in which the data varies.\n",
        "* They are orthogonal (uncorrelated) to each other.\n",
        "\n",
        "### **Eigenvalues**\n",
        "\n",
        "* Tell us **how much variance** each eigenvector (component) captures.\n",
        "* Larger eigenvalue ‚Üí more important principal component.\n",
        "* They allow us to rank components by significance.\n",
        "\n",
        "In short:\n",
        "\n",
        "* **Eigenvectors = directions of maximum variance**\n",
        "* **Eigenvalues = amount of variance in those directions**\n",
        "\n",
        "\n",
        "#  **Why Are Eigenvalues and Eigenvectors Important in PCA?**\n",
        "\n",
        "### **1. They determine the principal components**\n",
        "\n",
        "PCA picks the top *k* eigenvectors (based on largest eigenvalues).\n",
        "These become the new axes after transformation.\n",
        "\n",
        "\n",
        "### **2. They help reduce dimensionality**\n",
        "\n",
        "Eigenvalues indicate how much information (variance) each component contains.\n",
        "\n",
        "Example:\n",
        "If the first 2 components capture 95% of total variance, we can safely reduce from, say, 20 dimensions to 2.\n",
        "\n",
        "\n",
        "### **3. They ensure uncorrelated components**\n",
        "\n",
        "Because eigenvectors of the covariance matrix are orthogonal:\n",
        "\n",
        "* Principal components do not overlap in the information they describe.\n",
        "* Models often perform better with uncorrelated features.\n",
        "\n",
        "\n",
        "### **4. They help remove noise**\n",
        "\n",
        "Small eigenvalues correspond to directions with very little variance, often noise.\n",
        "\n",
        "By removing components with small eigenvalues, PCA:\n",
        "\n",
        "* Simplifies data\n",
        "* Enhances signal\n",
        "* Reduces overfitting\n",
        "\n",
        "\n",
        "#  Example\n",
        "\n",
        "Imagine your data lies mostly along a diagonal line on a 2D plane:\n",
        "\n",
        "1. PCA finds the eigenvector aligned with this line ‚Üí **1st principal component**.\n",
        "2. The eigenvalue for this direction is large ‚Üí lots of variance.\n",
        "3. A second eigenvector perpendicular to the line captures little variance (small eigenvalue).\n",
        "\n",
        "So you keep the first component and drop the second ‚Üí dimensionality reduced from 2D to 1D.\n"
      ],
      "metadata": {
        "id": "tKmvULl_J4JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "P6--MLGBOJCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "\n",
        "# **How KNN and PCA Complement Each Other**\n",
        "\n",
        "KNN and PCA are commonly used together because PCA helps address several weaknesses of KNN. The combination improves both **accuracy** and **efficiency**.\n",
        "\n",
        "\n",
        "#  **1. PCA reduces dimensionality ‚Üí makes KNN more effective**\n",
        "\n",
        "KNN suffers from the **curse of dimensionality** because distance becomes meaningless when there are too many features.\n",
        "\n",
        "PCA:\n",
        "\n",
        "* Reduces the number of features\n",
        "* Removes noise\n",
        "* Keeps the most informative directions of variance\n",
        "\n",
        "This makes distance-based algorithms like KNN **more reliable**.\n",
        "\n",
        "\n",
        "#  **2. PCA removes correlated and redundant features**\n",
        "\n",
        "If many features are correlated:\n",
        "\n",
        "* KNN may overemphasize those dimensions\n",
        "* Distances can become distorted\n",
        "\n",
        "PCA transforms the data into **uncorrelated (orthogonal)** components, which leads to more meaningful distance calculations.\n",
        "\n",
        "This directly improves KNN performance.\n",
        "\n",
        "#  **3. PCA reduces noise ‚Üí KNN makes better neighbor choices**\n",
        "\n",
        "KNN has no internal mechanism for ignoring noise, since it relies entirely on raw distances.\n",
        "\n",
        "PCA removes components with very small variance (often noise), helping KNN:\n",
        "\n",
        "* Avoid misleading neighbors\n",
        "* Improve generalization\n",
        "* Reduce overfitting\n",
        "\n",
        "\n",
        "#  **4. PCA improves KNN speed**\n",
        "\n",
        "KNN is slow at prediction time because it must compute distances to all points.\n",
        "\n",
        "Reducing dimensions via PCA:\n",
        "\n",
        "* Decreases computational cost\n",
        "* Makes KNN faster\n",
        "* Reduces memory usage\n",
        "\n",
        "This is especially important for large datasets.\n",
        "\n",
        "\n",
        "#  **5. PCA enables better visualization before applying KNN**\n",
        "\n",
        "Using PCA to project high-dimensional data into 2D or 3D helps you:\n",
        "\n",
        "* Visualize class separation\n",
        "* Detect clusters\n",
        "* Identify whether KNN is appropriate\n",
        "\n",
        "This supports better model design.\n",
        "\n",
        "\n",
        "#  **6. PCA + KNN is a common ML pipeline**\n",
        "\n",
        "A typical pipeline looks like:\n",
        "\n",
        "1. **Scale the data** (important for both PCA and KNN)\n",
        "2. **Apply PCA** to reduce to *k* principal components\n",
        "3. **Use KNN** on the transformed feature space\n",
        "\n",
        "This pipeline often provides:\n",
        "\n",
        "* Higher accuracy\n",
        "* Better generalization\n",
        "* Faster prediction\n",
        "* More stable distance metrics\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FXdREDUyJ79x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "lZexwkw1OQvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()        # loads dataset directly from sklearn\n",
        "X = data.data             # features\n",
        "y = data.target           # labels\n",
        "\n",
        "# Split into training/testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn_no_scale.predict(X_test)\n",
        "\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "print(\"Accuracy WITHOUT Scaling:\", accuracy_no_scale)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_with_scale = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy WITH Scaling:\", accuracy_with_scale)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR-LPSOLO0ia",
        "outputId": "a43d5d37-9b59-4829-923c-c725e1a72d21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.7222222222222222\n",
            "Accuracy WITH Scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n"
      ],
      "metadata": {
        "id": "IMCoaP9aJ_Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio of Each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMuvTdrRQDmB",
        "outputId": "50ae8cac-c790-4c0f-d92a-8d50ffc36778"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of Each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n"
      ],
      "metadata": {
        "id": "6YHLX1U-KEcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(\"Accuracy on ORIGINAL scaled dataset:\", accuracy_original)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"Accuracy on PCA (2 components) dataset:\", accuracy_pca)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq7b968fQR_x",
        "outputId": "affee9c6-03ea-48c9-ce61-5fc95e832512"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on ORIGINAL scaled dataset: 0.9444444444444444\n",
            "Accuracy on PCA (2 components) dataset: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "1yW9QpknKHP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "def evaluate_knn(metric_name, **knn_kwargs):\n",
        "    knn = KNeighborsClassifier(**knn_kwargs)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n--- KNN (metric = {metric_name}) ---\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names, digits=4))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
        "                       index=[f\"true_{t}\" for t in target_names],\n",
        "                       columns=[f\"pred_{t}\" for t in target_names]))\n",
        "    return acc\n",
        "\n",
        "acc_euclidean = evaluate_knn(\"euclidean\", n_neighbors=5, metric=\"minkowski\", p=2)\n",
        "\n",
        "acc_manhattan = evaluate_knn(\"manhattan\", n_neighbors=5, metric=\"manhattan\")\n",
        "\n",
        "print(\"\\nSummary of accuracies:\")\n",
        "print(f\"Euclidean (p=2) : {acc_euclidean:.4f}\")\n",
        "print(f\"Manhattan (L1)   : {acc_manhattan:.4f}\")\n",
        "\n",
        "ks = [1,3,5,7,9,11]\n",
        "results = []\n",
        "for k in ks:\n",
        "    knn_e = KNeighborsClassifier(n_neighbors=k, metric=\"minkowski\", p=2).fit(X_train_scaled, y_train)\n",
        "    knn_m = KNeighborsClassifier(n_neighbors=k, metric=\"manhattan\").fit(X_train_scaled, y_train)\n",
        "    acc_e = accuracy_score(y_test, knn_e.predict(X_test_scaled))\n",
        "    acc_m = accuracy_score(y_test, knn_m.predict(X_test_scaled))\n",
        "    results.append((k, acc_e, acc_m))\n",
        "\n",
        "print(\"\\nAccuracy by k:\")\n",
        "print(pd.DataFrame(results, columns=[\"k\", \"euclidean_acc\", \"manhattan_acc\"]).set_index(\"k\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7xgicZhQl_G",
        "outputId": "c4984077-1a69-4e54-9f12-97e8974e8259"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- KNN (metric = euclidean) ---\n",
            "Accuracy: 0.9444\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0     1.0000    1.0000    1.0000        18\n",
            "     class_1     1.0000    0.8571    0.9231        21\n",
            "     class_2     0.8333    1.0000    0.9091        15\n",
            "\n",
            "    accuracy                         0.9444        54\n",
            "   macro avg     0.9444    0.9524    0.9441        54\n",
            "weighted avg     0.9537    0.9444    0.9448        54\n",
            "\n",
            "Confusion Matrix:\n",
            "              pred_class_0  pred_class_1  pred_class_2\n",
            "true_class_0            18             0             0\n",
            "true_class_1             0            18             3\n",
            "true_class_2             0             0            15\n",
            "\n",
            "--- KNN (metric = manhattan) ---\n",
            "Accuracy: 0.9815\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0     1.0000    1.0000    1.0000        18\n",
            "     class_1     1.0000    0.9524    0.9756        21\n",
            "     class_2     0.9375    1.0000    0.9677        15\n",
            "\n",
            "    accuracy                         0.9815        54\n",
            "   macro avg     0.9792    0.9841    0.9811        54\n",
            "weighted avg     0.9826    0.9815    0.9816        54\n",
            "\n",
            "Confusion Matrix:\n",
            "              pred_class_0  pred_class_1  pred_class_2\n",
            "true_class_0            18             0             0\n",
            "true_class_1             0            20             1\n",
            "true_class_2             0             0            15\n",
            "\n",
            "Summary of accuracies:\n",
            "Euclidean (p=2) : 0.9444\n",
            "Manhattan (L1)   : 0.9815\n",
            "\n",
            "Accuracy by k:\n",
            "    euclidean_acc  manhattan_acc\n",
            "k                               \n",
            "1        0.962963       0.962963\n",
            "3        0.944444       0.962963\n",
            "5        0.944444       0.981481\n",
            "7        0.944444       0.981481\n",
            "9        0.962963       0.981481\n",
            "11       0.962963       0.962963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5yurBEQoKRd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "*   Use PCA to reduce dimensionality\n",
        "*   Decide how many components to keep\n",
        "*  Use KNN for classification post-dimensionality reduction\n",
        "*  Evaluate the model\n",
        "* Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "\n",
        "\n",
        "You are working with a **high-dimensional gene expression dataset**, where the number of features (genes) is extremely large, and the number of samples (patients) is small. This causes **overfitting** and poor generalization in traditional machine-learning models.\n",
        "\n",
        "Below is how to design a **PCA ‚Üí KNN classification pipeline** and justify it as a robust real-world biomedical approach.\n",
        "\n",
        "\n",
        "# üîπ **1. Using PCA to Reduce Dimensionality**\n",
        "\n",
        "Gene expression datasets often contain **thousands of genes**. Many of these:\n",
        "\n",
        "* Are correlated\n",
        "* Contain noise\n",
        "* Provide redundant information\n",
        "\n",
        "To reduce dimensionality:\n",
        "\n",
        "1. **Standardize the data** (PCA requires scaling)\n",
        "2. Apply PCA to transform the original features into a smaller number of **principal components**\n",
        "3. These components capture the majority of biological signal while reducing noise\n",
        "\n",
        "PCA helps combat:\n",
        "\n",
        "* Overfitting\n",
        "* Noise sensitivity\n",
        "* The curse of dimensionality\n",
        "\n",
        "\n",
        "# üîπ **2. Deciding How Many Components to Keep**\n",
        "\n",
        "We select the number of principal components using:\n",
        "\n",
        "### **Explained Variance Ratio**\n",
        "\n",
        "Choose enough components to capture **90‚Äì95% of total variance**, ensuring minimal information loss.\n",
        "\n",
        "### **Scree / Elbow Plot**\n",
        "\n",
        "Pick the point where adding more components yields diminishing returns.\n",
        "\n",
        "###  **Cross-validation**\n",
        "\n",
        "Evaluate classification accuracy for different numbers of components and choose the number that performs best.\n",
        "\n",
        "This ensures that PCA keeps only the **most meaningful biological patterns**.\n",
        "\n",
        "\n",
        "# üîπ **3. Using KNN for Classification After PCA**\n",
        "\n",
        "After PCA transforms the dataset:\n",
        "\n",
        "* The data becomes **lower-dimensional, noise-reduced, and uncorrelated**.\n",
        "* KNN becomes more effective because distances are now meaningful.\n",
        "* Overfitting risk decreases dramatically.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Train a **KNN classifier** on the PCA-transformed features.\n",
        "2. Tune **k** using cross-validation.\n",
        "3. Predict cancer type for new patients.\n",
        "\n",
        "KNN is simple, interpretable, and benefits greatly from PCA.\n",
        "\n",
        "\n",
        "# üîπ **4. Model Evaluation**\n",
        "\n",
        "Use:\n",
        "\n",
        "### **Stratified train‚Äìtest split or cross-validation**\n",
        "\n",
        "Ensures all cancer types are represented.\n",
        "\n",
        "###  **Metrics**\n",
        "\n",
        "* Accuracy\n",
        "* Precision, recall, F1-score\n",
        "* Confusion matrix\n",
        "* ROC-AUC (one-vs-rest for multiclass)\n",
        "\n",
        "### **Repeated cross-validation**\n",
        "\n",
        "Because sample sizes are small, repeating K-fold CV yields more stable estimates.\n",
        "\n",
        "\n",
        "# üîπ **5. Justification to Stakeholders (Real-World Biomedical Context)**\n",
        "\n",
        "This PCA ‚Üí KNN pipeline is a strong choice because:\n",
        "\n",
        "###  **Addresses overfitting in high-dimensional biomedical data**\n",
        "\n",
        "PCA extracts the core biological signal and removes noise.\n",
        "\n",
        "###  **Produces stable, reproducible results**\n",
        "\n",
        "PCA reduces variance, ensuring the model doesn‚Äôt learn patient-specific noise.\n",
        "\n",
        "###  **Improves interpretability**\n",
        "\n",
        "Principal components can be mapped back to gene groups, allowing domain experts to study which pathways drive classification.\n",
        "\n",
        "###  **Computationally efficient**\n",
        "\n",
        "KNN on reduced PCA features is much faster and more scalable.\n",
        "\n",
        "###  **Widely used in genomics**\n",
        "\n",
        "Dimensionality reduction + distance-based methods are standard in cancer subtype analysis, clustering, and biomarker discovery.\n",
        "\n",
        "###  **Transparent & trusted**\n",
        "\n",
        "Unlike black-box deep learning, PCA + KNN is interpretable and easy to validate in clinical settings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dPvipKQBKWpC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fhxOpv5Jcug",
        "outputId": "e3597740-d3e6-4e51-cb39-dbe71b29f0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PCA components retained: 10\n",
            "Explained variance ratio: [0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019]\n",
            "\n",
            "Accuracy after PCA + KNN: 0.9629629629629629\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.88      1.00      0.94        15\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.96      0.97      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Question 10: PCA + KNN Pipeline for High-Dimensional Data\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Number of PCA components retained:\", pca.n_components_)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy after PCA + KNN:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    }
  ]
}